{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81022860-b1d3-4f89-8815-f75968b90ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from torchvision.transforms import ToTensor\n",
    "from jax.scipy.special import logsumexp\n",
    "#import jax.opt as jopt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "030a73ff-59c6-4a10-9eec-20ad367381e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2285895361 1501764800]\n",
      " [1518642379 4090693311]\n",
      " [ 433833334 4221794875]\n",
      " [ 839183663 3740430601]]\n",
      "[6, 100, 100]\n",
      "[100, 100, 1]\n"
     ]
    }
   ],
   "source": [
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n, ))\n",
    "    \n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    \n",
    "    print(keys)\n",
    "    print(sizes[:-1])\n",
    "    print(sizes[1:])\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], \n",
    "                                                            sizes[1:], \n",
    "                                                            keys)]\n",
    "\n",
    "layer_sizes = [6, 100, 100, 1]\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 10000\n",
    "n_targets = 1\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5019448-139f-4b92-8a67-d0e3f117bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "def selu(x):\n",
    "    return jax.nn.selu(x)\n",
    "\n",
    "def predict(params, image):\n",
    "    activations = image\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80449b77-8574-482f-8402-157b1dcfca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_flattened_images = random.normal(random.PRNGKey(1), \n",
    "                                        (10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4845f8aa-b558-4d84-9318-61ab6514c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_predict = vmap(predict, in_axes = (None, 0))\n",
    "batched_predict_alt = vmap(predict, in_axes = (0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72a85151-4784-49fb-9543-fda1e9090d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_preds = batched_predict(params, random_flattened_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0143c36a-6c76-4398-a666-74836c525948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "880e05eb-7829-46a0-a71e-c608f8512533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot(x, k, dtype=jnp.float32):\n",
    "#   \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "#   return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "# def accuracy(params, images, targets):\n",
    "#     target_class = jnp.argmax(targets, axis=1)\n",
    "#     predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "#     return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "# def loss(params, images, targets):\n",
    "#     preds = batched_predict(params, images)\n",
    "#     return -jnp.mean(preds * targets)\n",
    "def huber_loss(target: float, pred: float, delta: float = 1.0) -> float:\n",
    "    \"\"\"Huber loss.\n",
    "\n",
    "    Args:\n",
    "    target: ground truth\n",
    "    pred: predictions\n",
    "    delta: radius of quadratic behavior\n",
    "    Returns:\n",
    "    loss value\n",
    "\n",
    "    References:\n",
    "    https://en.wikipedia.org/wiki/Huber_loss\n",
    "    \"\"\"\n",
    "    abs_diff = jnp.abs(target - pred)\n",
    "    return jnp.where(abs_diff > delta,\n",
    "                   delta * (abs_diff - .5 * delta),\n",
    "                   0.5 * abs_diff ** 2)\n",
    "\n",
    "def loss(params, features, targets, delta = 1.):\n",
    "    print('preds shape')\n",
    "    preds = batched_predict(params, features)\n",
    "    print(preds.shape)\n",
    "    my_huber_loss = jnp.mean(huber_loss(targets, preds, delta = delta))\n",
    "    print('loss shape')\n",
    "    print(my_huber_loss.shape)\n",
    "    return my_huber_loss\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1795b034-f69f-41c3-914b-257eacf9c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/users/afengler/data/' + \\\n",
    "                       'proj_lan_pipeline/LAN_scripts/data/lan_mlp/' + \\\n",
    "                       'training_data_0_nbins_0_n_2000/ddm/'\n",
    "file_list = [folder + file_ for file_ in os.listdir(folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70955666-9570-4aef-9781-dba7299fd166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 6)\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "example_file = pickle.load(open(file_list[0], 'rb'))\n",
    "print(example_file['data'].shape)\n",
    "print(len(file_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0feff248-f872-45a9-89dc-8968b8bb3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_per_epoch = np.floor(len(file_list) * example_file['data'].shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6429363-2bb8-4292-a106-59b812bb6329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b411812-f6a7-48ec-bb38-964ca9d247cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class DatasetTorch(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                file_IDs, \n",
    "                batch_size = 1,\n",
    "                label_prelog_cutoff_low = 1e-7,\n",
    "                label_prelog_cutoff_high = None\n",
    "                ):\n",
    "\n",
    "        # Initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.file_IDs = file_IDs\n",
    "        self.indexes = np.arange(len(self.file_IDs))\n",
    "        self.label_prelog_cutoff_low = label_prelog_cutoff_low\n",
    "        self.label_prelog_cutoff_high = label_prelog_cutoff_high\n",
    "        self.tmp_data = None\n",
    "        self.__init_file_shape()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor((len(self.file_IDs) * self.file_shape_dict['inputs'][0]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "\n",
    "        # Find list of IDs\n",
    "        if index % self.batches_per_file == 0 or self.tmp_data == None:\n",
    "            self.__load_file(file_index = self.indexes[index // self.batches_per_file])\n",
    "\n",
    "        # Generate data\n",
    "        batch_ids = np.arange(((index % self.batches_per_file) * self.batch_size), ((index % self.batches_per_file) + 1) * self.batch_size, 1)\n",
    "        X, y = self.__data_generation(batch_ids)\n",
    "        return X.astype(jnp.float32), y.astype(jnp.float32)\n",
    "\n",
    "    def __load_file(self, file_index):\n",
    "        self.tmp_data = pickle.load(open(self.file_IDs[file_index], 'rb'))\n",
    "        shuffle_idx = np.random.choice(self.tmp_data['data'].shape[0], size = self.tmp_data['data'].shape[0], replace = True)\n",
    "        self.tmp_data['data'] = self.tmp_data['data'][shuffle_idx, :]\n",
    "        self.tmp_data['labels'] = self.tmp_data['labels'][shuffle_idx]       \n",
    "        return\n",
    "\n",
    "    def __init_file_shape(self):\n",
    "        init_file = pickle.load(open(self.file_IDs[0], 'rb'))\n",
    "        #print('Init file shape: ', init_file['data'].shape, init_file['labels'].shape)\n",
    "        \n",
    "        self.file_shape_dict = {'inputs': init_file['data'].shape, 'labels': init_file['labels'].shape}\n",
    "        self.batches_per_file = int(self.file_shape_dict['inputs'][0] / self.batch_size)\n",
    "        self.input_dim = self.file_shape_dict['inputs'][1]\n",
    "        \n",
    "        if len(self.file_shape_dict['labels']) > 1:\n",
    "            self.label_dim = self.file_shape_dict['labels'][1]\n",
    "        else:\n",
    "            self.label_dim = 1\n",
    "        return\n",
    "\n",
    "    def __data_generation(self, batch_ids = None):\n",
    "        #print('passed datageneration')\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.squeeze(self.tmp_data['data'][batch_ids, :]) #tmp_file[batch_ids, :-1]\n",
    "        y = self.tmp_data['labels'][batch_ids] #tmp_file[batch_ids, -1]\n",
    "        #print(X.shape)\n",
    "#         print(type(self.tmp_data['data']))\n",
    "#         print(type(self.tmp_data['labels']))\n",
    "#         print(type(X))\n",
    "#         print(type(y))\n",
    "        if self.label_prelog_cutoff_low is not None:\n",
    "            y[y < np.log(self.label_prelog_cutoff_low)] = np.log(self.label_prelog_cutoff_low)\n",
    "        \n",
    "        if self.label_prelog_cutoff_high is not None:\n",
    "            y[y > np.log(self.label_prelog_cutoff_high)] = np.log(self.label_prelog_cutoff_high)\n",
    "#         print(type(y))\n",
    "#         print(type(X))\n",
    "#         print(type(y))\n",
    "#         print(type(x))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a32c5e7-0fe9-4413-8cc3-e9a9e3f30900",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_training_dataset = DatasetTorch(file_IDs = file_list,\n",
    "                                      batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30fae61a-c909-49f4-9635-a3b6b89cb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "    \n",
    "torch_training_dataloader = torch.utils.data.DataLoader(torch_training_dataset,\n",
    "                                                        shuffle = True,\n",
    "                                                        batch_size = 1,\n",
    "                                                        num_workers = 0,\n",
    "                                                        collate_fn = numpy_collate,\n",
    "                                                        pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70f79866-5cd5-49a8-8377-0e6ef1bbd3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in torch_training_dataloader:\n",
    "    print(jnp.squeeze(x).shape)\n",
    "    print(jnp.squeeze(y).shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fb36ea1-669e-4444-a4b8-4b1c5946394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "179.55225157737732\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "170.04355478286743\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "177.06561493873596\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "172.14164972305298\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "166.09915828704834\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "167.80193614959717\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "166.56317710876465\n",
      "100 of 2020.0  epochs ran\n",
      "200 of 2020.0  epochs ran\n",
      "300 of 2020.0  epochs ran\n",
      "400 of 2020.0  epochs ran\n",
      "500 of 2020.0  epochs ran\n",
      "600 of 2020.0  epochs ran\n",
      "700 of 2020.0  epochs ran\n",
      "800 of 2020.0  epochs ran\n",
      "900 of 2020.0  epochs ran\n",
      "1000 of 2020.0  epochs ran\n",
      "1100 of 2020.0  epochs ran\n",
      "1200 of 2020.0  epochs ran\n",
      "1300 of 2020.0  epochs ran\n",
      "1400 of 2020.0  epochs ran\n",
      "1500 of 2020.0  epochs ran\n",
      "1600 of 2020.0  epochs ran\n",
      "1700 of 2020.0  epochs ran\n",
      "1800 of 2020.0  epochs ran\n",
      "1900 of 2020.0  epochs ran\n",
      "2000 of 2020.0  epochs ran\n",
      "168.68546152114868\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    cnt = 0\n",
    "    for x, y in torch_training_dataloader:\n",
    "        #y = one_hot(y, n_targets)\n",
    "        params = update(params, jnp.squeeze(x), jnp.squeeze(y))\n",
    "        cnt += 1\n",
    "        if (cnt % 100) == 0:\n",
    "            print(cnt, 'of', n_steps_per_epoch, ' epochs ran')\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(epoch_time)\n",
    "#     for x, y in training_generator:\n",
    "#         my_loss = \n",
    "    #train_acc = accuracy(params, train_images, train_labels)\n",
    "    #test_acc = accuracy(params, test_images, test_labels)\n",
    "#     print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "#     print(\"Training set accuracy {}\".format(train_acc))\n",
    "#     print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539022f5-c66c-4651-a82d-b199f2920da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "# This is applied when the __getitem__ method in the dataset (mnist_dataset below)\n",
    "# is invoked\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        #print(pic)\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32)).astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba7050-f7eb-4400-8585-d33f54b1409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dataset, using torch datasets\n",
    "mnist_dataset = MNIST('data/mnist/', \n",
    "                      download=True,\n",
    "                      transform=FlattenAndCast())\n",
    "training_generator = NumpyLoader(mnist_dataset, \n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8487501e-0ee1-4197-a652-24323a1dbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full train dataset ( for checking accuracy while training)\n",
    "train_images = np.array(mnist_dataset.data[500:, :, :]).reshape(len(mnist_dataset.data[500:]), - 1) \n",
    "train_labels = one_hot(np.array(mnist_dataset.targets[500:]), n_targets)                                                                  \n",
    "\n",
    "# Get test dataset\n",
    "test_images = np.array(mnist_dataset.data[:500, :, :]).reshape(len(mnist_dataset.data[:500]), - 1) \n",
    "test_labels = one_hot(np.array(mnist_dataset.targets[:500]), n_targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19eb2006-cc72-409f-8bf0-ea940bfd4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 5.89 sec\n",
      "Training set accuracy 0.942874014377594\n",
      "Test set accuracy 0.9500000476837158\n",
      "Epoch 1 in 5.57 sec\n",
      "Training set accuracy 0.9526386857032776\n",
      "Test set accuracy 0.956000030040741\n",
      "Epoch 2 in 5.16 sec\n",
      "Training set accuracy 0.960084080696106\n",
      "Test set accuracy 0.9600000381469727\n",
      "Epoch 3 in 5.27 sec\n",
      "Training set accuracy 0.9652605652809143\n",
      "Test set accuracy 0.9620000720024109\n",
      "Epoch 4 in 5.37 sec\n",
      "Training set accuracy 0.9689244031906128\n",
      "Test set accuracy 0.968000054359436\n",
      "Epoch 5 in 5.19 sec\n",
      "Training set accuracy 0.9720168709754944\n",
      "Test set accuracy 0.9720000624656677\n",
      "Epoch 6 in 5.29 sec\n",
      "Training set accuracy 0.974907636642456\n",
      "Test set accuracy 0.9720000624656677\n",
      "Epoch 7 in 5.29 sec\n",
      "Training set accuracy 0.9772941470146179\n",
      "Test set accuracy 0.9740000367164612\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for x, y in training_generator:\n",
    "        y = one_hot(y, n_targets)\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5c542-8726-4d1b-ac18-e3146370cb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanfactory",
   "language": "python",
   "name": "lanfactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
